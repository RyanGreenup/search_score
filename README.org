#+TITLE:Searching text with C

* How it works
  Pull out all the ordered pair of words, then use TF-IDF weighting on
  those, this way there's no stemming or anything to worry about.

* Performance Considerations
  With python calling the program takes 1.7 times longer than it takes
  to run, there are overheads to calling a program, this will be far
  far less for C, but, I should incorporate the loop and the sorting
  inside C for the best performance.

  
  |                 | C       | Python |
  |-----------------+---------+--------|
  | Execution Time  | 1*10^-3 |    0.3 |
  | Evaluation Time | 4*10^-5 |    0.5 |
  |-----------------+---------+--------|
  | Diff            | 1 ms    |  0.2 s |
  #+begin_src bash
    time echo 'some words here' | ./search.py 'some words'   
  #+end_src

  
* Note Size
  There seems to be a difference in performance given the length of a
  note, given that Wikipedia appears to have articles that are
  [[https://en.wikipedia.org/wiki/Wikipedia:Words_per_article][on average]] 400-1000 characters long, that is what is reasonable to
  target with my notes.

  Here are some notes of varying length:

| Words | Name                               | Performance         |
|-------+------------------------------------+---------------------|
| 27000 | Statistics All Export.org          | quite good actually |
|   950 | Comparison of Markdown Editors     |                     |
|   500 | cosine similarity                  |                     |
|   100 | Create node vertex graphs in latex |                     |


This could be because the search query needs to be closer in length?


Actually by looking at the number of words that are meaningful in
longer documents, it becomes clear that the reason my searching
algorithm isn't useful is because of the amount of words not related
to the search, particularly in =org= buffers where words like =PROPERTIES=
and =ID= and =CUSTOM= are dominant.

this can be inspected by doing:

#+begin_src bash
cat AbstractAlgebraNotes.org | tr -c '[:alnum:]' '[\n*]' | sort | uniq -c | sort -nr | head  -30
#+end_src

Experiments searching for the most frequent words indicates that the
search algorithm works very well in that case, so the solution to this
is definitely to implement some form of TF-IDF weighting to account
for words that are not important.

This might be hard in C, perhaps I should consider Go?

From experimentation, there are about 50, 000 unique 4-tuples in a
document, and this just about represents the upper bounds of what is
feasible for this approach, so, using 4-tuples, is not the ideal solution.
